---
layout: default
title: Chris Gavin
mathjax: true
---
<div class="blurb">
	<h1>github of Chris Gavin</h1>
	<p>Computer Science PhD Student at the University of Colorado</p>

<h1 style="text-align:left;">Current Research</h1>

<p class="norm">The video is a bit shaky. The video is in a playlist with some of my 5+ year old projects.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/videoseries?list=PLWABySa_8guOkCh04UxyeQDPNO7DnXcJr" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p class="norm">I increased the grasp success rate on these m6 screws to roughly 95%, although it's a first working version, using a <br> Convolutional Neural Network to predict grasp success. Our previous baseline success rate without deep learning was ~80% percent, but <br>that baseline is <r>somewhat arbitrary as it is based on how well it was hardcoded.<br>
I'm currently writing a paper on this.<br></p>

<br><br>
<h1 style="text-align:left;">TRPO</h1>
<h2 style="text-align:left;">Trust Region Policy Optimization Notes</h2>

<!-- \begin{document} -->
<!-- \maketitle -->
<!-- % Minorize Maxmize Algorithm -->

<!-- % Uses a surrogate function that is  -->
<div style="text-align:center;">

     <br>
    Reinforcement Learning (RL) algorithms typically try to maximize the expected discounted reward.
     <br>
    Expected discounted reward:&ensp; $\eta(\pi_\theta)=\displaystyle\mathop{\mathbb{E}_{so,ao,...} }\Big[ \sum\limits_{t=0}^{\infty}\gamma^t r_t \Big]$<br>
    TRPO uses a version of the Minorize Maximize (MM) algorithm to formulate the optimization problem.<br>
    The MM algorithm uses a surrogate function that is a lower bound for $\eta$.<br>
    It approximates $\eta$ under the current policy. The surrogate function is quadratic and therefore is easy to optimize.<br>
    The surrogate function for TRPO is $M=L(\theta)-C*KL$<br>
    <!-- % Using MM they iteratively optimize M w.r.t the policy. Then M is reevaluated with the new policy -->
     <br>
    TRPO uses an advantage function rather than the reward as it lowers variance.<br>
    Advantage function: $A_\pi(s,a)=Q_\pi(s,a)-V_\pi(s)$<br>
    They use an estimate of $A: \hat{A}_t$<br>
     <br>
    Importance Sampling:<br>
    TRPO Uses a form of importance sampling in the objective function<br>
    Samples are taken from the old policy and the ratio of the new policy at those samples divided by the old policy at those samples is used to <br>represent the policy portion of the objective function. This makes policies that are better than the old policy with the same samples more valuable.
    <br>
    <br>

    This leaves us with an optimization problem like this:<br>
     <br>
    $M=L(\theta)-C*KL$
     <br>
    Loss Function $L(\theta): \hat{\mathbb{E}_t }\Big[\dfrac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old} }(a_t|s_t)}\hat{A}_t\Big]$<br>
     <br>
    Constraint:&nbsp;&nbsp;$\hat{\mathbb{E}_t }\Big[KL\Big[\pi_{\theta_{old}}\Big(\cdot|s_t\Big),\pi_{\theta}\Big(\cdot|s_t\Big)\Big]\Big] &ensp;\leq&ensp; \delta$

    <br>
    <br>
    Solving the optimization problem:<br>

    L is approximated to the first order and the KL divergence to the second.<br><br>

    $L_{\theta_k}(\theta)\approx g^T(\theta - \theta_k)$ <br>
    $D_{KL}(\theta||\theta_k)\approx \tfrac{1}{2}(\theta - \theta_k)^T H(\theta - \theta_k)$ <br><br>


    The quadratic part of L is small compared to KL term and the Hessian of L isn't positive semidefinite, but the Hessian of the KL term is.<br><br>
    The problem with approximations is now:<br>
    $\theta_{k+1}=argmax_\theta&ensp;g^T(\theta - \theta_k)<br>
    &emsp; s.t.&nbsp;\tfrac{1}{2}(\theta - \theta_k)^T H(\theta - \theta_k) \leq \delta $<br>
    <!-- % \phantom{x}\hspace{3ex} for fancy spaing -->
    Solving analytically leads to:<br>
    $\theta_{k+1}-\theta_k=\sqrt{\tfrac{2\delta}{g^T H^{-1}g} }H^{-1}g $<br>

    The term on the right hand side is referred to as the Natural Policy Gradient.<br>
    Let $\beta = \sqrt{\tfrac{2\delta}{g^T H^{-1}g} }$<br>
    Now $\theta_{k+1}-\theta_k=\tfrac{1}{\beta} H^{-1}g $<br>
    Let $\beta = 1$. They now solve $H(\theta_{k+1}-\theta_k)= g$ &nbsp; approximately using the conjugate gradient method <br>
    to avoid computing the Hessian and it's inverse.<br>
    $\beta$ was a scalar and didn't effect direction, but it is now used with line search to find a good step size<br>
    <br><br>
  </div>
<!-- Picutre of overall algorithm -->
<!-- \end{document} -->
  <!-- $\nabla_\theta$ -->
</div><!-- /.blurb -->
